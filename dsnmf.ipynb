{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "relu = lambda x: 0.5 * (x + abs(x))\n",
    "\n",
    "def floatX(x):\n",
    "    return np.asarray(x, dtype=theano.config.floatX)\n",
    "\n",
    "def appr_seminmf(M, r):\n",
    "    \"\"\"\n",
    "        Approximate Semi-NMF factorisation. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        M: array-like, shape=(n_features, n_samples)\n",
    "        r: number of components to keep during factorisation\n",
    "    \"\"\"\n",
    "    \n",
    "    if r < 2:\n",
    "        raise ValueError(\"The number of components (r) has to be >=2.\")\n",
    "\n",
    "    A, S, B = svds(M, r-1)\n",
    "    S = np.diag(S)\n",
    "    A = np.dot(A, S)\n",
    " \n",
    "    m, n = M.shape\n",
    " \n",
    "    for i in range(r-1):\n",
    "        if B[i, :].min() < (-B[i, :]).min():\n",
    "            B[i, :] = -B[i, :]\n",
    "            A[:, i] = -A[:, i]\n",
    "            \n",
    "            \n",
    "    if r == 2:\n",
    "        U = np.concatenate([A, -A], axis=1)\n",
    "    else:\n",
    "        An = -np.sum(A, 1).reshape(A.shape[0], 1)\n",
    "        U = np.concatenate([A, An], 1)\n",
    "    \n",
    "    V = np.concatenate([B, np.zeros((1, n))], 0)\n",
    "\n",
    "    if r>=3:\n",
    "        V -= np.minimum(0, B.min(0))\n",
    "    else:\n",
    "        V -= np.minimum(0, B)\n",
    "\n",
    "    return U, V\n",
    "    \n",
    "def adam(loss, params, learning_rate=0.001, beta1=0.9,\n",
    "         beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"Adam updates\n",
    "    Adam updates implemented as in [1]_.\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_or_grads : symbolic expression or list of expressions\n",
    "        A scalar loss expression, or a list of gradient expressions\n",
    "    params : list of shared variables\n",
    "        The variables to generate update expressions for\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    beta_1 : float\n",
    "        Exponential decay rate for the first moment estimates.\n",
    "    beta_2 : float\n",
    "        Exponential decay rate for the second moment estimates.\n",
    "    epsilon : float\n",
    "        Constant for numerical stability.\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict\n",
    "        A dictionary mapping each parameter to its update expression\n",
    "    Notes\n",
    "    -----\n",
    "    The paper [1]_ includes an additional hyperparameter lambda. This is only\n",
    "    needed to prove convergence of the algorithm and has no practical use\n",
    "    (personal communication with the authors), it is therefore omitted here.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n",
    "           Adam: A Method for Stochastic Optimization.\n",
    "           arXiv preprint arXiv:1412.6980.\n",
    "    \"\"\"\n",
    "         \n",
    "    all_grads = theano.grad(loss, params)\n",
    "    t_prev = theano.shared(floatX(0.))\n",
    "    updates = OrderedDict()\n",
    "\n",
    "    for param, g_t in zip(params, all_grads):\n",
    "        m_prev = theano.shared(param.get_value() * 0.)\n",
    "        v_prev = theano.shared(param.get_value() * 0.)\n",
    "        t = t_prev + 1\n",
    "        m_t = beta1*m_prev + (1-beta1)*g_t\n",
    "        v_t = beta2*v_prev + (1-beta2)*g_t**2\n",
    "        a_t = learning_rate*T.sqrt(1-beta2**t)/(1-beta1**t)\n",
    "        step = a_t*m_t/(T.sqrt(v_t) + epsilon)\n",
    "\n",
    "        updates[m_prev] = m_t\n",
    "        updates[v_prev] = v_t\n",
    "        updates[param] = param - step\n",
    "\n",
    "    updates[t_prev] = t\n",
    "    return updates\n",
    "\n",
    "\n",
    "def init_weights(X, num_components, svd_init=True):\n",
    "    if svd_init:\n",
    "        return appr_seminmf(X, num_components)\n",
    "\n",
    "    Z = 0.08 * np.random.rand(X.shape[0], num_components)\n",
    "    H = 0.08 * np.random.rand(num_components, X.shape[1])\n",
    "\n",
    "    return Z, H\n",
    "\n",
    "\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "rng = RandomStreams()\n",
    "\n",
    "def dropout(x, p=0):\n",
    "    if p == 0:\n",
    "        return x\n",
    "    else:\n",
    "        p = 1 - p\n",
    "        x /= p\n",
    "\n",
    "        return x * rng.binomial(x.shape, p=p, dtype=theano.config.floatX)\n",
    "        \n",
    "\n",
    "class DSNMF(object):\n",
    "\n",
    "    def __init__(self, data, layers, verbose=False, l1_norms=[], pretrain=True, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        :param data: array-like, shape=(n_samples, n_features)\n",
    "        :param layers: list, shape=(n_layers) containing the size of each of the layers\n",
    "        :param verbose: boolean\n",
    "        :param l1_norms: list, shape=(n_layers) the l1-weighting of each of the layers\n",
    "        :param pretrain: pretrain layers using svd\n",
    "        \"\"\"\n",
    "        H = data.T\n",
    "        \n",
    "        assert len(layers) > 0, \"You have to provide a positive number of layers.\"\n",
    "\n",
    "        params = []\n",
    "\n",
    "        for i, l in enumerate(layers, start=1):\n",
    "            print('Pretraining {}th layer [{}]'.format(i, l), end='\\r')\n",
    "\n",
    "            Z, H = init_weights(H, l, svd_init=pretrain)\n",
    "\n",
    "            params.append(theano.shared(floatX(Z), name='Z_%d' % (i)))\n",
    "\n",
    "        params.append(theano.shared(floatX(H), name='H_%d' % len(layers)))\n",
    "\n",
    "        self.params = params\n",
    "        self.layers = layers\n",
    "        \n",
    "        cost = ((data.T - self.get_h(-1))**2).sum()\n",
    "        \n",
    "        for norm, param in zip(l1_norms, params):\n",
    "            cost += ((abs(param)) * norm).sum()\n",
    "    \n",
    "        H = relu(self.params[-1])\n",
    "        \n",
    "        updates = adam(cost, params, learning_rate=learning_rate)\n",
    "\n",
    "        self.cost = cost\n",
    "        self.train_fun = theano.function([], cost, updates=updates)\n",
    "        self.get_features = theano.function([], H)\n",
    "\n",
    "        self.get_reconstruction = theano.function([], self.get_h(-1))\n",
    "\n",
    "    def finetune_features(self):\n",
    "\n",
    "        updates = adam(self.cost, self.params[-1:])\n",
    "        self.train_fun = theano.function([], self.cost, updates=updates)\n",
    "\n",
    "    def get_param_values(self):\n",
    "        return [p.get_value() for p in self.params]\n",
    "\n",
    "    def set_param_values(self, values):\n",
    "        params = self.params\n",
    "\n",
    "        if len(params) != len(values):\n",
    "            raise ValueError(\"mismatch: got %d values to set %d parameters\" %\n",
    "                            (len(values), len(params)))\n",
    "\n",
    "        for p, v in zip(params, values):\n",
    "            if p.get_value().shape[0] != v.shape[0]:\n",
    "                raise ValueError(\"mismatch: parameter has shape %r but value to \"\n",
    "                             \"set has shape %r\" %\n",
    "                             (p.get_value().shape, v.shape))\n",
    "            else:\n",
    "                p.set_value(v)\n",
    "\n",
    "    def get_h(self, layer_num, have_dropout=False):\n",
    "        h = relu(self.params[-1])\n",
    "        \n",
    "        if have_dropout:\n",
    "            h = dropout(h, p=.1)\n",
    "            \n",
    "        for z in reversed(self.params[1:-1][:]):\n",
    "            h = relu(z.dot(h))\n",
    "\n",
    "        if layer_num == -1:\n",
    "            h = self.params[0].dot(h)\n",
    "        \n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
